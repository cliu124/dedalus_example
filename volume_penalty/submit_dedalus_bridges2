#!/bin/bash
### partition 1: RM [default,use one or more full nodes], 
### partition 2: RM-shared [use only part of one node] 128 cpu per node
### partition 3: RM-512 [use one or more full 512GB-memory nodes]
#SBATCH --partition=RM-shared # 
#SBATCH --nodes=1
#SBATCH --ntasks=128
#SBATCH --time=1:00:00
###SBATCH --mail-user=chang_liu@uconn.edu      # Destination email address
#SBATCH --mail-type=ALL                       # Event(s) that triggers email notification
#SBATCH --job-name=dedalus_job
#SBATCH --output=dedalus_output_%j
export SLURM_EXPORT_ENV=ALL
#export I_MPI_FABRICS=shm,tcp

##the slurm number to restart simulation... This need full state to be stored.
SUBMITDIR=$SLURM_SUBMIT_DIR
WORKDIR=/ocean/projects/phy240052p/cliu4/testing/dedalus_$SLURM_JOB_ID
mkdir -p "$WORKDIR" && cp -r *.py "$WORKDIR" && cp submit_dedalus_bridges2 "$WORKDIR" && cd "$WORKDIR" || exit -1

# submit with base envs from local computer
source activate base
conda activate dedalus3
mpiexec -n $SLURM_NTASKS python3 main_channel_flow_turbulent_micro_channel_flow_penalty.py

cd "$SUBMITDIR" && cp dedalus_output_$SLURM_JOB_ID "$WORKDIR"

